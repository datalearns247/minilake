{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lF2hIje6Xajk"
   },
   "source": [
    "# Lab 02 - Pemrosesan Data Dengan DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bggy7DeBX-1c"
   },
   "source": [
    "Notebook ini menunjukkan bagaimana melakukan pemrosesan data menggunakan pyspark DataFrame. \n",
    "\n",
    "Operasi yang akan dilakukan adalah : \n",
    "1. [Membuat Dataframe](#01.Membuat-DataFrame)\n",
    "2. [Memeriksa sekilas sebuah DataFrame](#2.1-Memeriksa-sekilas-DataFrame)\n",
    "3. [Memfilter dataframe berdasar kolom dan baris](#2.2-Filtering)\n",
    "4. [Menampilkan unique value](#2.3-Unique-value)\n",
    "5. [Agregasi](#2.4-Agregasi)\n",
    "6. [Transformasi DataFrame](#2.5-Transformasi-DataFrame)\n",
    "7. [Data Enrichment : Join DataFrame](#2.6-Data-enrichment---Join)\n",
    "\n",
    "Import package yang dibutuhkan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "p0CyPWlHU5oL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('attempt to write a readonly database')).History will not be written to the database.\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3s0yTjp_X2JP"
   },
   "source": [
    "Create spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "RFeguv1rU6lq"
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Eksplorasi DataFrame').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aiotjcgbz9gv"
   },
   "source": [
    "# 01.Membuat DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R4puMNX2WfRN"
   },
   "source": [
    "DataFrame dapat dibuat dengan banyak cara, di antaranya :\n",
    "- Dari python object, misalnya array/list, dictionary, pandas dataframe, dll\n",
    "- Dari file : csv, json, dll\n",
    "- Dari HDFS\n",
    "- Dari RDD\n",
    "- dll."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dOf-4dxAgOsv"
   },
   "source": [
    "## 1.1. Create From Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ScgatWyqepcH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------+\n",
      "|     province|density|\n",
      "+-------------+-------+\n",
      "|  DKI JAKARTA|  15328|\n",
      "|   JAWA BARAT|   1320|\n",
      "|  JAWA TENGAH|   1030|\n",
      "|DI YOGYAKARTA|   1174|\n",
      "|   JAWA TIMUR|    813|\n",
      "|       BANTEN|   1237|\n",
      "+-------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mydata = (('DKI JAKARTA',15328),\n",
    "('JAWA BARAT',1320),\n",
    "('JAWA TENGAH',1030),\n",
    "('DI YOGYAKARTA',1174),\n",
    "('JAWA TIMUR',813),\n",
    "('BANTEN',1237))\n",
    "\n",
    "df_from_array = spark.createDataFrame(mydata).toDF(\"province\", \"density\")\n",
    "\n",
    "df_from_array.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XYN1nYAiWx6u"
   },
   "source": [
    "## 1.2. Create from Pandas DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H4ksYmW7ZCXF"
   },
   "source": [
    "Dowload file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "FSEQn5OJ4vnK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-08-06 04:54:38--  https://raw.githubusercontent.com/urfie/SparkSQL-dengan-Hive/main/datasets/penduduk2015.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 120 [text/plain]\n",
      "Saving to: ‘dataset/penduduk2015.csv.2’\n",
      "\n",
      "penduduk2015.csv.2  100%[===================>]     120  --.-KB/s    in 0s      \n",
      "\n",
      "2025-08-06 04:54:38 (2.23 MB/s) - ‘dataset/penduduk2015.csv.2’ saved [120/120]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -P dataset https://raw.githubusercontent.com/urfie/SparkSQL-dengan-Hive/main/datasets/penduduk2015.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AzgGjBoCZEqP"
   },
   "source": [
    "Create pandas dataframe dari file csv tersebut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "34owT-3uAGca"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pddf = pd.read_csv('dataset/penduduk2015.csv')\n",
    "pddf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3P8yoarCZL5o"
   },
   "source": [
    "Ubah ke Spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r7Hyq3iuO-RF"
   },
   "outputs": [],
   "source": [
    "df_from_pandas = spark.createDataFrame(pddf)\n",
    "df_from_pandas.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XKco8E9FgidD"
   },
   "source": [
    "## 1.3. Create from csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5IwfM7QBZRlS"
   },
   "source": [
    "Kita juga bisa me-load langsung file csv tersebut ke Spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zQ3yUkvjlKqN"
   },
   "outputs": [],
   "source": [
    "df_from_csv = spark.read.csv(\"dataset/penduduk2015.csv\", header=True)\n",
    "df_from_csv.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n83qkTxzrrma"
   },
   "source": [
    "## 1.4. Create from JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i_NWvf1E52v2"
   },
   "outputs": [],
   "source": [
    "!wget -P dataset https://raw.githubusercontent.com/urfie/SparkSQL-dengan-Hive/main/datasets/penduduk2015.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eTcgb1HEevnh"
   },
   "source": [
    "Tampilkan isi file dengan perintah `cat`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3nI8eugSjORM"
   },
   "outputs": [],
   "source": [
    "!cat dataset/penduduk2015.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cmMxONptZmPU"
   },
   "source": [
    "Untuk membaca multiline JSON, set parameter `multiline` = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XhVBi_ndruRA"
   },
   "outputs": [],
   "source": [
    "dfj = spark.read.json(\"dataset/penduduk2015.json\", multiLine=True)\n",
    "dfj.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pzpuNpb8myW3"
   },
   "source": [
    "# 2.Explorasi DataFrame\n",
    "\n",
    "Dalam latihan ini kita akan mencoba berbagai operasi pada Spark DataFrame untuk melakukan eksplorasi data.\n",
    "\n",
    "Kita akan menggunakan data kepadatan penduduk per propinsi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-qO8foHlWexn"
   },
   "outputs": [],
   "source": [
    "!wget -P dataset https://raw.githubusercontent.com/urfie/SparkSQL-dengan-Hive/main/datasets/indonesia2013-2015.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3qD7ifpcl4ue"
   },
   "source": [
    "Kita gunakan magic command untuk melihat ukuran dan isi file (karena file kita cukup kecil)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4YKCOaiwmKO9"
   },
   "outputs": [],
   "source": [
    "%ls -al dataset/indonesia2013-2015.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kEX7VuFZl498"
   },
   "outputs": [],
   "source": [
    "%cat dataset/indonesia2013-2015.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iw-Y0fIMl29B"
   },
   "source": [
    "Karena data yang kita load sudah bersih, kita akan set inferSchema = True agar Spark menyesuaikan tipe kolom dengan datanya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4tQqD2VYm7ZR"
   },
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"dataset/indonesia2013-2015.csv\",header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vbwg-RH5m1JG"
   },
   "source": [
    "### 2.1 Memeriksa sekilas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oTGwnupfKBMU"
   },
   "source": [
    "#### Menampilkan beberapa baris\n",
    "\n",
    "Biasanya kita menampilkan beberapa baris data untuk mengecek format dan konten dataframe yang kita buat.\n",
    "\n",
    "Untuk menampilkan beberapa baris dari dataframe, kita bisa gunakan perintah ``show(n)`` untuk menampilkan n baris pertama, atau ``first()`` untuk menampilkan 1 baris pertama saja."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y-ccShKgnGBw"
   },
   "outputs": [],
   "source": [
    "df.show(5)\n",
    "df.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fX7rzHbOQRyv"
   },
   "source": [
    "#### Menampilkan jumlah kolom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CbUMZTJ8QSYF"
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GkyOlEULP_g-"
   },
   "source": [
    "#### Menampilkan total records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YtSRF5xFQBmc"
   },
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "99-M00F5nKdO"
   },
   "source": [
    "#### Menampilkan skema\n",
    "\n",
    "Untuk menampilkan skema dataframe, gunakan fungsi `printSchema()`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WOlh-X3nnL62"
   },
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b_rKrAAenlC0"
   },
   "source": [
    "Akses atribut `columns` untuk menampilkan list nama kolom\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "juHaBiLq_ldO"
   },
   "source": [
    "Akses atribut `dtypes` untuk menampilkan list nama kolom beserta data type masing-masing kolom tersebut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "49CkcSSNnuRH"
   },
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OpPxH6tlnY5R"
   },
   "source": [
    "#### Menampilkan summary statistik\n",
    "\n",
    "Fungsi `describe()` digunakan untuk menampilkan summary statistik dari seluruh kolom.\n",
    "\n",
    "Jangan lupa memanggil fungsi `show()` untuk menampilkan hasilnya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S_WjXzh-ncSu"
   },
   "outputs": [],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WWykQ_0KnpLT"
   },
   "source": [
    "Untuk menampilkan statistik dari salah satu kolom saja, gunakan nama kolom yang akan ditampilkan sebagai parameter. Misalnya `describe('column1')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "feq2JOTOn0dq"
   },
   "outputs": [],
   "source": [
    "df.describe(\"density\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DYDiKb4Rn_0c"
   },
   "source": [
    "### 2.2 Filtering\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y3BTu53eBAqp"
   },
   "source": [
    "Kita dapat melakukan filtering terhadap spark dataframe, berdasar kolom atau baris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-om-OrZquQQo"
   },
   "source": [
    "#### Memilih kolom tertentu\n",
    "\n",
    "Untuk menampilkan kolom tertentu, digunakan fungsi `select('nama_kolom')`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QdQzWlmvoJXv"
   },
   "outputs": [],
   "source": [
    "df.select(\"province\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a1x_c-b9pcWc"
   },
   "source": [
    "Untuk memilih beberapa kolom, gunakan tanda koma sebagai pemisah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SQbyEyDCpekb"
   },
   "outputs": [],
   "source": [
    "df.select(\"province\",\"density\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ghJrZSCpoo7C"
   },
   "source": [
    "#### Memilih records / baris\n",
    "\n",
    "Untuk memilih baris dengan kondisi tertentu, gunakan fungsi `filter(<kondisi>)`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xwTmG_QyoQHM"
   },
   "outputs": [],
   "source": [
    "df.filter(df.density > 1000).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8c4790g0r7GK"
   },
   "source": [
    "Untuk menggunakan kondisi berupa operasi string, dapat digunakan fungsi-fungsi dari `pyspark.sql.Column` yang terkait string, misalnya `contains()`, `startswith()`, `endswith()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8VL5e4DFofWH"
   },
   "outputs": [],
   "source": [
    "df.filter(df.province.contains('TENGGARA')).show(5)\n",
    "df.filter(df.province.startswith('SU')).show(10)\n",
    "df.filter(df.province.endswith('BARAT')).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2u7x8thvs2yp"
   },
   "source": [
    "Tersedia juga fungsi `like()` yang serupa dengan SQL statement *like*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V3qDU-rYs7kc"
   },
   "outputs": [],
   "source": [
    "df.filter(df.province.like('SU%')).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sWaHUzxHtSeg"
   },
   "source": [
    "Atau dapat juga menggunakan regex, dengan fungsi `rlike()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OVjEkO-vtUtC"
   },
   "outputs": [],
   "source": [
    "df.filter(df.province.rlike('[A-Z]*TA$')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "114ZKh-PuArH"
   },
   "source": [
    "Dapat juga menggunakan filter berdasar list, dengan fungsi `isin()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0FNg0NkIt-2g"
   },
   "outputs": [],
   "source": [
    "df.filter(df.timezone.isin('WIT','WITA')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YKWMWATGW9oN"
   },
   "outputs": [],
   "source": [
    "df.filter(df.province.isin('BALI','PAPUA')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U_zzv1_aYXBe"
   },
   "source": [
    "Untuk menggunakan beberapa kondisi sekaligus, menggunakan tanda `&` untuk AND dan `|` untuk OR, dengan masing-masing kondisi dilingkupi tanda kurung `()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5dAZ7aDoYalg"
   },
   "outputs": [],
   "source": [
    "df.filter((df.timezone.isin('WIT','WITA')) & (df.year == 2013)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DZb03bk5QKwo"
   },
   "source": [
    "### 2.3 Unique value\n",
    "\n",
    "Untuk menampilkan nilai unik dari dataframe, digunakan fungsi `distinct()`.\n",
    "\n",
    "Nilai unik di sini adalah kombinasi nilai dari seluruh kolom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sAIvwxbpQbj-"
   },
   "outputs": [],
   "source": [
    "df.distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u_2VKvj1QnVc"
   },
   "source": [
    "Untuk menampilkan nilai unik dari kolom tertentu, tulis nama kolom yang dimaksud sebagai parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EjlP69w-QqaV"
   },
   "outputs": [],
   "source": [
    "df.select('timezone').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VmmswyxeX_ci"
   },
   "outputs": [],
   "source": [
    "df.select('year','timezone').distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P3Uie0m1uVeJ"
   },
   "source": [
    "#### Menghapus duplikasi data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "65nP6e1lgTFE"
   },
   "source": [
    "Untuk menghapus record duplikat, gunakan `dropDuplicates(subset)` atau `drop_duplicates(subset)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5feLEyAJuZPw"
   },
   "outputs": [],
   "source": [
    "df.dropDuplicates(['province', 'timezone']).show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rcLgEXz-PlvN"
   },
   "source": [
    "### 2.4 Agregasi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R0kFRECNRqBL"
   },
   "source": [
    "#### Group by column\n",
    "\n",
    "Untuk mengelompokkan berdasar kolom, gunakan perintah `groupBy('nama_kolom')`\n",
    "\n",
    "Untuk mengelompokkan berdasar lebih dari 1 kolom, gunakan tanda koma sebagai pemisah nama kolom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oVa4FSWws8Xo"
   },
   "outputs": [],
   "source": [
    "df.groupBy(\"timezone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IKsy1iphtWwe"
   },
   "outputs": [],
   "source": [
    "df.groupBy(\"timezone\",\"year\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3bzjnDEQtdiI"
   },
   "source": [
    "Perintah `groupBy()` menghasilkan obyek `GroupedData` yang belum bisa ditampilkan.\n",
    "\n",
    "Biasanya setelah pengelompokan, kita melakukan operasi sumarisasi data. Kita terapkan operasi tersebut pada objek hasil groupBy dengan memanggil fungsi yang dibutuhkan. Misalnya `count()` atau `max('namakolom')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B9aXQsIWFt3O"
   },
   "outputs": [],
   "source": [
    "df.groupBy('timezone').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0vuMZmwCZ10Q"
   },
   "outputs": [],
   "source": [
    "df.groupBy('timezone').max('density').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Kr85hEQJl8a"
   },
   "source": [
    "Kita juga bisa menggunakan fungsi `agg()` untuk melakukan agregasi. Terutama jika kita ingin melakukan lebih dari 1 operasi agregat.\n",
    "\n",
    "Kita bisa menggunakan fungsi `alias()` untuk memberi nama kolom hasil agregasi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_d2I29cpHV_Q"
   },
   "outputs": [],
   "source": [
    "df.groupBy(\"timezone\").agg(F.max('density')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HcOH3Ry-aYry"
   },
   "outputs": [],
   "source": [
    "df.groupBy(\"timezone\").agg(F.avg('density').alias('avg_density'), \\\n",
    "                           F.min('density').alias('min_density'), \\\n",
    "                           F.max('density').alias('max_density')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cci7rwBA0XiD"
   },
   "source": [
    "#### Order By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4LMTfZnTiZMI"
   },
   "outputs": [],
   "source": [
    "df.groupBy(\"timezone\") \\\n",
    "  .mean(\"density\") \\\n",
    "  .orderBy(\"timezone\",ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JD-e2D1X0lKq"
   },
   "source": [
    "#### Agregasi dengan filter / kondisi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RBy0oEPGhQAL"
   },
   "outputs": [],
   "source": [
    "df.groupBy(\"timezone\") \\\n",
    "  .mean(\"density\") \\\n",
    "  .where(df.timezone.contains('WIT')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TtQRWTYttuXh"
   },
   "source": [
    "#### Filter hasil agregat (SQL stat **HAVING**)\n",
    "\n",
    "Untuk memfilter berdasar hasil agregasi (semacam perintah **HAVING** di SQL), lakukan dalam 2 langkah.\n",
    "1. Lakukan `groupBy` + `agg` dan beri nama kolom hasil agregat dengan `alias`\n",
    "2. gunakan fungsi `filter(kondisi)` pada kolom hasil agregasi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B9VnDO8Dw3AO"
   },
   "outputs": [],
   "source": [
    "df_agg = df.groupBy(\"timezone\", \"province\") \\\n",
    "  .agg(F.avg(\"density\").alias(\"avg_density\")) \\\n",
    "  .where(df.timezone.contains('WIT'))\n",
    "\n",
    "df_agg.filter(df_agg.avg_density > 50).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t_PLmohQyBSM"
   },
   "source": [
    "### 2.5 Transformasi DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Xu2MZwP0Hab"
   },
   "source": [
    "#### Kolom baru berupa nilai konstan\n",
    "\n",
    "Untuk menambahkan kolom baru ke dalam dataframe, kita bisa menggunakan perintah `withColumn()`\n",
    "\n",
    "Sedangkan untuk menambahkan sebuah nilai konstan, kita bisa menggunakan fungsi `lit(nilai_konstan)` dari `pyspark.sql.functions`, yang berfungsi membuat kolom dari nilai literal/konstan.\n",
    "\n",
    "Misalnya kita ingin menambahkan kolom **status** yang bernilai 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RfFvAGjqAJ3n"
   },
   "outputs": [],
   "source": [
    "df.withColumn('status', F.lit(1)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1LizYZ9dBYzI"
   },
   "source": [
    "#### Kolom baru dari kolom yang ada\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4rh9fTgaBdsS"
   },
   "outputs": [],
   "source": [
    "df.withColumn('tahun-1', df.year-1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RMydt8Imc2KE"
   },
   "outputs": [],
   "source": [
    "df.withColumn('year', df.year-1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yaDbu8YSCZqh"
   },
   "source": [
    "#### Kondisional\n",
    "\n",
    "Untuk menambahkan kolom berdasar beberapa kondisi, gunakan `when` dan `otherwise` (jika perlu).\n",
    "\n",
    "Perhatikan bahwa `when` yang pertama adalah fungsi dalam `pyspark.sql.functions`, sedangkan `when` yang berikutnya adalah fungsi `when` pada object kolom (`pyspark.sql.Column`)\n",
    "\n",
    "Fungsi `otherwise` adalah kondisi *else* atau kondisi selain yang disebutkan pada *when*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t70gfGAYBtSh"
   },
   "outputs": [],
   "source": [
    "df.withColumn('timezone_code', F.when(df.timezone == 'WIT', 1).\n",
    "              when(df.timezone == 'WITA', 2).\n",
    "              otherwise(3)).show(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "52CxmiOt0dwW"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KO7h6k6iHEXr"
   },
   "source": [
    "### 2.6 Data enrichment - Join\n",
    "\n",
    "Perintah ntuk melakukan join adalah sebagai berikut :\n",
    "\n",
    "`df1.join(df2, on=[columname], how=’left’)`\n",
    "\n",
    "Where :\n",
    "-    `df1` − Dataframe1.\n",
    "-    `df2` – Dataframe2.\n",
    "-    `on` − nama kolom yang akan digunakan untuk join.\n",
    "-    `how` – type of join needs to be performed – `left`, `right`, `outer`, `inner`. Defaultnya adalah inner join.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1kaJk2F5E4WM"
   },
   "source": [
    "Create dataframe yang akan digunakan untuk join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3zpT210OGDMa"
   },
   "outputs": [],
   "source": [
    "damdata = ((\"SUMATERA SELATAN\",2),\n",
    "(\"SULAWESI TENGAH\",2),\n",
    "(\"SULAWESI SELATAN\",2),\n",
    "(\"SUMATERA BARAT\",3),\n",
    "(\"RIAU\",3),\n",
    "(\"LAMPUNG\",3),\n",
    "(\"NUSA TENGGARA TIMUR\",4),\n",
    "(\"BENGKULU\",8),\n",
    "(\"SUMATERA UTARA\",10),\n",
    "(\"JAWA TIMUR\",12),\n",
    "(\"JAWA TENGAH\",35),\n",
    "(\"JAWA BARAT\",49))\n",
    "\n",
    "df_dam = spark.createDataFrame(damdata).toDF(\"province\", \"dam_num\")\n",
    "\n",
    "df_dam.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8QN1QKE8_Dp9"
   },
   "source": [
    "Join dataframe kepadatan penduduk dengan dataframe jumlah bendungan, berdasarkan nama propinsi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4BxH6wBdG7bY"
   },
   "outputs": [],
   "source": [
    "df.join(df_dam, on=['province'], how='left').show(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d_3UfzMTIDzg"
   },
   "source": [
    "Untuk melakukan left join, gunakan parameter `how`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8yLuRcy4IEV3"
   },
   "outputs": [],
   "source": [
    "df.join(df_dam, on=['province'], how='left').show(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tGrlD5ashACP"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
